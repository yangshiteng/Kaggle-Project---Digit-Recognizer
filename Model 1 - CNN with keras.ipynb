{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a 5 layers Sequential Convolutional Neural Network for digits recognition trained on MNIST dataset. I choosed to build it with keras API (Tensorflow backend) which is very intuitive. Firstly, I will prepare the data (handwritten digits images) then i will focus on the CNN modeling and evaluation.\n",
    "\n",
    "I achieved 99.671% of accuracy with this CNN trained in 2h30 on a single CPU (i5 2500k). For those who have a >= 3.0 GPU capabilites (from GTX 650 - to recent GPUs), you can use tensorflow-gpu with keras. Computation will be much much faster !!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "D:\\Anacoda3\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train = pd.read_csv(r\"C:\\Users\\yangs\\Dropbox\\Data Scientist Study\\Project\\Project 2 - Didit Recognizer\\train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\yangs\\Dropbox\\Data Scientist Study\\Project\\Project 2 - Didit Recognizer\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_train = train[\"label\"]\n",
    "\n",
    "# Drop 'label' column\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "\n",
    "# free some space\n",
    "del train \n",
    "\n",
    "g = sns.countplot(Y_train)\n",
    "\n",
    "Y_train.value_counts()\n",
    "\n",
    "# We have similar counts for the 10 digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for null and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       784\n",
       "unique        1\n",
       "top       False\n",
       "freq        784\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data\n",
    "X_train.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       784\n",
       "unique        1\n",
       "top       False\n",
       "freq        784\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I check for corrupted images (missing values inside).\n",
    "\n",
    "##### There is no missing values in the train and test dataset. So we can safely go ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "We perform a grayscale normalization to reduce the effect of illumination's differences.\n",
    "\n",
    "Moreover the CNN converg faster on [0..1] data than on [0..255]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train = X_train / 255.0\n",
    "test = test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "test = test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and test images (28px x 28px) has been stock into pandas.Dataframe as 1D vectors of 784 values. We reshape all data to 28x28x1 3D matrices.\n",
    "\n",
    "#### Keras requires an extra dimension in the end which correspond to channels. MNIST images are gray scaled so it use only one channel. For RGB images, there is 3 channels, we would have reshaped 784px vectors to 28x28x3 3D matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "Y_train = to_categorical(Y_train, num_classes = 10)\n",
    "\n",
    "\n",
    "# Labels are 10 digits numbers from 0 to 9. We need to encode these lables to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0]).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training and valdiation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "random_seed = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and the validation set for the fitting\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choosed to split the train set in two parts : a small fraction (10%) became the validation set which the model is evaluated and the rest (90%) is used to train the model.\n",
    "\n",
    "Since we have 42 000 training images of balanced labels (see 2.1 Load data), a random split of the train set doesn't cause some labels to be over represented in the validation set. Be carefull with some unbalanced dataset a simple random split could cause inaccurate evaluation during the validation.\n",
    "\n",
    "To avoid that, you could use stratify = True option in train_test_split function (Only for >=0.17 sklearn versions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can get a better sense for one of these examples by visualising the image and looking at the label.\n",
    "\n",
    "# Some examples\n",
    "g = plt.imshow(X_train[0][:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model\n",
    "\n",
    "I used the Keras Sequential API, where you have just to add one layer at a time, starting from the input.\n",
    "\n",
    "The first is the convolutional (Conv2D) layer. It is like a set of learnable filters. I choosed to set 32 filters for the two firsts conv2D layers and 64 filters for the two last ones. Each filter transforms a part of the image (defined by the kernel size) using the kernel filter. The kernel filter matrix is applied on the whole image. Filters can be seen as a transformation of the image.\n",
    "\n",
    "The CNN can isolate features that are useful everywhere from these transformed images (feature maps).\n",
    "\n",
    "The second important layer in CNN is the pooling (MaxPool2D) layer. This layer simply acts as a downsampling filter. It looks at the 2 neighboring pixels and picks the maximal value. These are used to reduce computational cost, and to some extent also reduce overfitting. We have to choose the pooling size (i.e the area size pooled each time) more the pooling dimension is high, more the downsampling is important.\n",
    "\n",
    "Combining convolutional and pooling layers, CNN are able to combine local features and learn more global features of the image.\n",
    "\n",
    "Dropout is a regularization method, where a proportion of nodes in the layer are randomly ignored (setting their weights to zero) for each training sample. This drops randomly a proportion of the network and forces the network to learn features in a distributed way. This technique also improves generalization and reduces the overfitting.\n",
    "\n",
    "'relu' is the rectifier (activation function max(0,x). The rectifier activation function is used to add non linearity to the network.\n",
    "\n",
    "The Flatten layer is used to convert the final feature maps into a one single 1D vector. This flattening step is needed so that you can make use of fully connected layers after some convolutional/maxpool layers. It combines all the found local features of the previous convolutional layers.\n",
    "\n",
    "In the end i used the features in two fully-connected (Dense) layers which is just artificial an neural networks (ANN) classifier. In the last layer(Dense(10,activation=\"softmax\")) the net outputs distribution of probability of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the CNN model \n",
    "# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1)))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the optimizer and annealer\n",
    "\n",
    "Once our layers are added to the model, we need to set up a score function, a loss function and an optimisation algorithm.\n",
    "\n",
    "We define the loss function to measure how poorly our model performs on images with known labels. It is the error rate between the oberved labels and the predicted ones. We use a specific form for categorical classifications (>2 classes) called the \"categorical_crossentropy\".\n",
    "\n",
    "The most important function is the optimizer. This function will iteratively improve parameters (filters kernel values, weights and bias of neurons ...) in order to minimise the loss.\n",
    "\n",
    "I choosed RMSprop (with default values), it is a very effective optimizer. The RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive, monotonically decreasing learning rate. We could also have used Stochastic Gradient Descent ('sgd') optimizer, but it is slower than RMSprop.\n",
    "\n",
    "The metric function \"accuracy\" is used is to evaluate the performance our model. This metric function is similar to the loss function, except that the results from the metric evaluation are not used when training the model (only for evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the optimizer converge faster and closest to the global minimum of the loss function, i used an annealing method of the learning rate (LR).\n",
    "\n",
    "The LR is the step by which the optimizer walks through the 'loss landscape'. The higher LR, the bigger are the steps and the quicker is the convergence. However the sampling is very poor with an high LR and the optimizer could probably fall into a local minima.\n",
    "\n",
    "Its better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function.\n",
    "\n",
    "To keep the advantage of the fast computation time with a high LR, i decreased the LR dynamically every X steps (epochs) depending if it is necessary (when accuracy is not improved).\n",
    "\n",
    "With the ReduceLROnPlateau function from Keras.callbacks, i choose to reduce the LR by half if the accuracy is not improved after 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy\n",
    "batch_size = 86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "\n",
    "In order to avoid overfitting problem, we need to expand artificially our handwritten digit dataset. We can make your existing dataset even larger. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit.\n",
    "\n",
    "For example, the number is not centered The scale is not the same (some who write with big/small numbers) The image is rotated...\n",
    "\n",
    "Approaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. Some popular augmentations people use are grayscales, horizontal flips, vertical flips, random crops, color jitters, translations, rotations, and much more.\n",
    "\n",
    "By applying just a couple of these transformations to our training data, we can easily double or triple the number of training examples and create a very robust model.\n",
    "\n",
    "The improvement is important:\n",
    "\n",
    "Without data augmentation i obtained an accuracy of 98.114%\n",
    "With data augmentation i achieved 99.67% of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without data augmentation i obtained an accuracy of 0.98114\n",
    "\n",
    "# history = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, \n",
    "#          validation_data = (X_val, Y_val), verbose = 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With data augmentation to prevent overfitting (accuracy 0.99286)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data augmentation, i choosed to :\n",
    "\n",
    "Randomly rotate some training images by 10 degrees\n",
    "Randomly Zoom by 10% some training images\n",
    "Randomly shift images horizontally by 10% of the width\n",
    "Randomly shift images vertically by 10% of the height\n",
    "I did not apply a vertical_flip nor horizontal_flip since it could have lead to misclassify symetrical numbers such as 6 and 9.\n",
    "\n",
    "Once our model is ready, we fit the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.gof.opt): Optimization failure due to: local_abstractconv_check\n",
      "ERROR (theano.gof.opt): node: AbstractConv2d{convdim=2, border_mode='half', subsample=(1, 1), filter_flip=True, imshp=(None, 1, 28, 28), kshp=(32, 1, 5, 5), filter_dilation=(1, 1), num_groups=1, unshared=False}(InplaceDimShuffle{0,3,1,2}.0, InplaceDimShuffle{3,2,0,1}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"D:\\Anacoda3\\lib\\site-packages\\theano\\gof\\opt.py\", line 2034, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"D:\\Anacoda3\\lib\\site-packages\\theano\\tensor\\nnet\\opt.py\", line 500, in local_abstractconv_check\n",
      "    node.op.__class__.__name__)\n",
      "theano.gof.opt.LocalMetaOptimizerSkipAssertionError: AbstractConv2d Theano optimization failed: there is no implementation available supporting the requested options. Did you exclude both \"conv_dnn\" and \"conv_gemm\" from the optimizer? If on GPU, is cuDNN available and does the GPU support it? If on CPU, do you have a BLAS library installed Theano can link against? On the CPU we do not support float16.\n",
      "\n"
     ]
    },
    {
     "ename": "LocalMetaOptimizerSkipAssertionError",
     "evalue": "AbstractConv2d Theano optimization failed: there is no implementation available supporting the requested options. Did you exclude both \"conv_dnn\" and \"conv_gemm\" from the optimizer? If on GPU, is cuDNN available and does the GPU support it? If on CPU, do you have a BLAS library installed Theano can link against? On the CPU we do not support float16.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLocalMetaOptimizerSkipAssertionError\u001b[0m      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-ff81a64588e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                               \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                               \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                               , callbacks=[learning_rate_reduction])\n\u001b[0m",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    331\u001b[0m                     \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmetrics_updates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                     \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train_function'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m                     **self._function_kwargs)\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[0;32m   1452\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m                 \u001b[0m_raise_invalid_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1454\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[0;32m   1419\u001b[0m                                         \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m                                         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1421\u001b[1;33m                                         **kwargs)\n\u001b[0m\u001b[0;32m   1422\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_is_metric'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m         self._metrics_function = theano.function(\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\theano\\compile\\function.py\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    315\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m                    output_keys=output_keys)\n\u001b[0m\u001b[0;32m    318\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\theano\\compile\\pfunc.py\u001b[0m in \u001b[0;36mpfunc\u001b[1;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m    484\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m                          output_keys=output_keys)\n\u001b[0m\u001b[0;32m    487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36morig_function\u001b[1;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m   1837\u001b[0m                   \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m                   \u001b[0moutput_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1839\u001b[1;33m                   name=name)\n\u001b[0m\u001b[0;32m   1840\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchange_flags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompute_test_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"off\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m             \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys, name)\u001b[0m\n\u001b[0;32m   1517\u001b[0m                         optimizer, inputs, outputs)\n\u001b[0;32m   1518\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1519\u001b[1;33m                     \u001b[0moptimizer_profile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1521\u001b[0m                 \u001b[0mend_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\theano\\gof\\opt.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, fgraph)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \"\"\"\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_requirements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\theano\\gof\\opt.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0morig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\theano\\gof\\opt.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fgraph)\u001b[0m\n\u001b[0;32m    249\u001b[0m                     \u001b[0mnb_nodes_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m                     \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m                     \u001b[0msub_prof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m                     \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                     \u001b[0msub_profs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_prof\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\theano\\gof\\opt.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0morig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\theano\\gof\\opt.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fgraph, start_from)\u001b[0m\n\u001b[0;32m   2141\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2142\u001b[0m                 \u001b[0mcurrent_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2143\u001b[1;33m                 \u001b[0mnb\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2144\u001b[0m             \u001b[0mloop_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\theano\\gof\\opt.py\u001b[0m in \u001b[0;36mprocess_node\u001b[1;34m(self, fgraph, node, lopt)\u001b[0m\n\u001b[0;32m   2037\u001b[0m                 self.failure_callback(e, self,\n\u001b[0;32m   2038\u001b[0m                                       \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2039\u001b[1;33m                                       lopt, node)\n\u001b[0m\u001b[0;32m   2040\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2041\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\theano\\gof\\opt.py\u001b[0m in \u001b[0;36mwarn_inplace\u001b[1;34m(exc, nav, repl_pairs, local_opt, node)\u001b[0m\n\u001b[0;32m   1931\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInconsistencyError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1932\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1933\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mNavigatorOptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnav\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl_pairs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1935\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\theano\\gof\\opt.py\u001b[0m in \u001b[0;36mwarn\u001b[1;34m(exc, nav, repl_pairs, local_opt, node)\u001b[0m\n\u001b[0;32m   1917\u001b[0m             \u001b[1;31m# We always crash on AssertionError because something may be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m             \u001b[1;31m# seriously wrong if such an exception is raised.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1919\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1920\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1921\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\theano\\gof\\opt.py\u001b[0m in \u001b[0;36mprocess_node\u001b[1;34m(self, fgraph, node, lopt)\u001b[0m\n\u001b[0;32m   2032\u001b[0m         \u001b[0mlopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlopt\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_opt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2033\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2034\u001b[1;33m             \u001b[0mreplacements\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2035\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2036\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfailure_callback\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda3\\lib\\site-packages\\theano\\tensor\\nnet\\opt.py\u001b[0m in \u001b[0;36mlocal_abstractconv_check\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[1;34m'do you have a BLAS library installed Theano can link against? '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m             \u001b[1;34m'On the CPU we do not support float16.'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m             node.op.__class__.__name__)\n\u001b[0m\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m optdb.register('AbstractConvCheck',\n",
      "\u001b[1;31mLocalMetaOptimizerSkipAssertionError\u001b[0m: AbstractConv2d Theano optimization failed: there is no implementation available supporting the requested options. Did you exclude both \"conv_dnn\" and \"conv_gemm\" from the optimizer? If on GPU, is cuDNN available and does the GPU support it? If on CPU, do you have a BLAS library installed Theano can link against? On the CPU we do not support float16."
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\n",
    "                              , callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-822cdb99b562>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Training loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"validation loss\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlegend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'best'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshadow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss and accuracy curves for training and validation \n",
    "\n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix can be very helpfull to see your model drawbacks.\n",
    "\n",
    "# I plot the confusion matrix of the validation results.\n",
    "\n",
    "\n",
    "# Look at confusion matrix \n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_val)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that our CNN performs very well on all digits with few errors considering the size of the validation set (4 200 images).\n",
    "\n",
    "However, it seems that our CNN has some little troubles with the 4 digits, hey are misclassified as 9. Sometime it is very difficult to catch the difference between 4 and 9 when curves are smooth.\n",
    "\n",
    "Let's investigate for errors.\n",
    "\n",
    "I want to see the most important errors . For that purpose i need to get the difference between the probabilities of real value and the predicted ones in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some error results \n",
    "\n",
    "# Errors are difference between predicted labels and true labels\n",
    "errors = (Y_pred_classes - Y_true != 0)\n",
    "\n",
    "Y_pred_classes_errors = Y_pred_classes[errors]\n",
    "Y_pred_errors = Y_pred[errors]\n",
    "Y_true_errors = Y_true[errors]\n",
    "X_val_errors = X_val[errors]\n",
    "\n",
    "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
    "            n += 1\n",
    "\n",
    "# Probabilities of the wrong predicted numbers\n",
    "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors \n",
    "most_important_errors = sorted_dela_errors[-6:]\n",
    "\n",
    "# Show the top 6 errors\n",
    "display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important errors are also the most intrigous.\n",
    "\n",
    "For those six case, the model is not ridiculous. Some of these errors can also be made by humans, especially for one the 9 that is very close to a 4. The last 9 is also very misleading, it seems for me that is a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict results\n",
    "results = model.predict(test)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "results = np.argmax(results,axis = 1)\n",
    "\n",
    "results = pd.Series(results,name=\"Label\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
